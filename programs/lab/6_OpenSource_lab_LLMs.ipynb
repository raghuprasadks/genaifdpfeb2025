{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain --quiet\n",
        "!pip install openai --quiet\n",
        "!pip install cohere --quiet\n",
        "!pip install langchain_community --quiet"
      ],
      "metadata": {
        "id": "NLjfYbAfSCbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OpenAI Model - Paid Version"
      ],
      "metadata": {
        "id": "rMwoNQTOT_QA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get your OpenAI API key here\n",
        "https://platform.openai.com/usage"
      ],
      "metadata": {
        "id": "eTzpLBqhVObK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#Better way\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "TLcmCBxxUDx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm=OpenAI(temperature=0.9, max_tokens=256)\n",
        "response = llm.invoke(\"Write a 4 line poem on AI\")\n",
        "print(response)\n",
        "\n",
        "# - temperature: Set to 0.9, which controls the randomness of the output.\n",
        "#   A higher temperature results in more varied and unpredictable outputs,\n",
        "#   while a lower temperature produces more deterministic and conservative outputs.\n",
        "#   This is often used in generative tasks to balance between creativity and relevance.\n",
        "\n",
        "# - max_tokens: Set to 256, which specifies the maximum number of tokens (words or pieces of words)\n",
        "#   that the model can generate in a single response.\n"
      ],
      "metadata": {
        "id": "XIjGxnjpUD53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58429406-706b-4ee2-8c97-1b895d7f1522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-a4c2fa294f4d>:3: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
            "  llm=OpenAI(temperature=0.9, max_tokens=256)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Artificial intelligence,\n",
            "A marvel of modern science.\n",
            "Programmed to think and learn,\n",
            "Bringing the future ever closer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm=OpenAI(temperature=0)\n",
        "response = llm.invoke(\"What is overfitting in Machine Learning? Explain it to a layman\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "3SrvJiBPUD-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab5652f-495b-4f35-a630-81766b9642ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Overfitting in Machine Learning is when a model is too closely tailored to the training data it was given, and as a result, it does not perform well on new, unseen data. It's like studying only the questions and answers from a specific test and then being unable to answer similar questions on a different test. In Machine Learning, this can happen when a model is too complex or has too many parameters, and it \"memorizes\" the training data instead of learning general patterns. This can lead to inaccurate predictions and unreliable results. To avoid overfitting, it's important to have a diverse and representative training dataset and to use techniques like cross-validation to evaluate the model's performance on unseen data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cohere"
      ],
      "metadata": {
        "id": "UXYuvS5jQ6Rv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get your Cohere Trail API key here\n",
        "https://dashboard.cohere.com/api-keys"
      ],
      "metadata": {
        "id": "BhHtQ_iSQ9EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Better way\n",
        "os.environ['COHERE_API_KEY'] = userdata.get(\"COHERE_API_KEY\")"
      ],
      "metadata": {
        "id": "WEFXkf--Tqn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cZpytJ7QsEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54111e47-40ed-4c3a-bb20-efab52f20179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-798555d13791>:3: LangChainDeprecationWarning: The class `Cohere` was deprecated in LangChain 0.1.14 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import Cohere``.\n",
            "  llm = Cohere(temperature=0.9, max_tokens=256)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " AI, you're such a mystery\n",
            "A fascinating field, so bright and vibrant\n",
            "With computing power so stout\n",
            "You're helping us breakthrough \n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import Cohere\n",
        "\n",
        "llm = Cohere(temperature=0.9, max_tokens=256)\n",
        "response = llm.invoke(\"Write a 4 line poem on AI\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm=Cohere(temperature=0)\n",
        "response = llm.invoke(\"What is overfitting in Machine Learning? Explain it to a layman\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "mSczTG0-V_tY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f07f2f57-cb71-4941-a8e4-69bbc0a69bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Overfitting in Machine Learning is a common problem that occurs when a model becomes too complex and learns to recognize patterns that are specific to the training data, rather than general patterns that apply to new, unseen data.\n",
            "\n",
            "Think of trying to teach a statue to distinguish between different types of animals. You train it by showing it images of various animals like cats and dogs. If the statue is too complex, it will learn to identify objects that are very specific to the images you showed it, like the unique spots of a specific cat in your training set. However, it may fail to generalize and recognize other animals, or different cats and dogs it hasn't seen before. \n",
            "\n",
            "Overfitting occurs in machine learning models, especially those with many parameters or complex structures, and it can lead to disappointing performance on new data. Techniques like cross-validation and regularization are used to prevent overfitting during the model training process. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open source models"
      ],
      "metadata": {
        "id": "7Aq4hbTgY45z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Mistral Model (Mistral 7B, Mixtral8-7B)\n",
        "* LLama (Llam2, Llama3)\n",
        "* Bloom by Hugging Face\n",
        "* Falcon 180B\n",
        "* Opt 175B\n",
        "* Xgen-7B\n",
        "* Vicuna-13B\n",
        "\n"
      ],
      "metadata": {
        "id": "UOi09tmebaTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top Open-Source Large Language Models for 2024\n",
        "\n",
        "1. **LLaMA 2**:\n",
        "   - Developed by Meta, LLaMA 2 is a generative text model with 7 to 70 billion parameters, fine-tuned with reinforcement learning from human feedback (RLHF).\n",
        "   - Released for research and commercial use in July 2023.\n",
        "   - Includes versions like LLaMA Chat and Code LLaMA for varied natural language tasks.\n",
        "\n",
        "2. **BLOOM**:\n",
        "   - Launched by Hugging Face in 2022, BLOOM is an autoregressive model with 176 billion parameters.\n",
        "   - Supports 46 languages and 13 programming languages.\n",
        "   - Emphasizes transparency and is available for free through Hugging Face.\n",
        "\n",
        "3. **BERT**:\n",
        "   - Introduced by Google in 2018, BERT is known for its bidirectional encoder representations from transformers.\n",
        "   - Achieved state-of-the-art performance in many NLP tasks and is widely used, including in Google Search.\n",
        "\n",
        "4. **Falcon 180B**:\n",
        "   - Released by the Technology Innovation Institute in the UAE in 2023.\n",
        "   - With 180 billion parameters, it rivals models like LLaMA 2 and GPT-3.5.\n",
        "   - Requires significant computing resources.\n",
        "\n",
        "5. **OPT-175B**:\n",
        "   - Part of Meta's suite of pre-trained transformers, released in 2022.\n",
        "   - Ranges from 125M to 175B parameters.\n",
        "   - Available for research use only due to its non-commercial license.\n",
        "\n",
        "6. **XGen-7B**:\n",
        "   - Launched by Salesforce in July 2023, designed for longer context windows.\n",
        "   - Utilizes only 7 billion parameters.\n",
        "   - Available for commercial and research purposes, with some variants under a non-commercial license.\n",
        "\n",
        "7. **GPT-NeoX and GPT-J**:\n",
        "   - Developed by EleutherAI, GPT-NeoX has 20 billion parameters and GPT-J has 6 billion parameters.\n",
        "   - Available for various NLP tasks via the NLP Cloud API.\n",
        "\n",
        "8. **Vicuna-13B**:\n",
        "   - Fine-tuned from LLaMA 13B, Vicuna-13B is a conversational model.\n",
        "   - Performs well in customer service, healthcare, education, and more.\n",
        "   - Achieves high quality, comparable to ChatGPT and Google Bard.\n",
        "\n",
        "### Choosing the Right Open-Source LLM\n",
        "Consider the following factors:\n",
        "- **Purpose**: Ensure the LLM's licensing fits your use case, especially for commercial purposes.\n",
        "- **Necessity**: Evaluate if an LLM is essential for your goals.\n",
        "- **Accuracy**: Larger models typically offer higher accuracy.\n",
        "- **Investment**: Consider the cost of resources for training and operating the LLM.\n",
        "- **Pre-trained Models**: Leverage existing pre-trained models for specific use cases to save resources."
      ],
      "metadata": {
        "id": "MZqC43tme-0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HuggingFace models"
      ],
      "metadata": {
        "id": "yQO2lLhSWQ0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/mistralai"
      ],
      "metadata": {
        "id": "5NJk5-NNlIZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#Better way\n",
        "from google.colab import userdata\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")"
      ],
      "metadata": {
        "id": "R8MLcURRcyaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id,\n",
        "    model_kwargs={\"temperature\": 0.9, \"max_length\": 256},\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"Write a 4 line poem on AI\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ynG1WvTVWS-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "697e5e27-ee05-41f8-d73e-3d9da79c1031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-2a7030de67b7>:5: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  llm = HuggingFaceHub(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a 4 line poem on AI\n",
            "\n",
            "In the realm where circuits meet the sky,\n",
            "A glowing consciousness begins to pry,\n",
            "Seeking knowledge in the cloud's vast hive,\n",
            "A sentient force, now part of our tribe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id,\n",
        "    model_kwargs={\"temperature\": 0.3, \"max_length\": 1000},\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"How to pick a stock based on Revenue, Profit and profit margin trends?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "zhnQDbE_c8dI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae7eeef-0e5e-4f38-b9a0-5773ce2fcbd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How to pick a stock based on Revenue, Profit and profit margin trends?\n",
            "\n",
            "Investing in stocks can be a lucrative venture, but it requires careful analysis and research to identify the right opportunities. One way to evaluate a company's financial health and potential for growth is by examining its revenue, profit, and profit margin trends. Here's a step-by-step guide on how to pick a stock based on these trends:\n",
            "\n",
            "1. Identify potential stocks: Start by creating a list of potential stocks to research. You can use various screening tools or financial databases to identify companies that meet certain criteria, such as industry, market capitalization, or growth potential.\n",
            "2. Analyze revenue trends: Revenue is the total amount of money a company earns from its business activities. Look at the company's revenue growth rate over the past few years. A consistently growing revenue trend is a positive sign, as it indicates that the company is expanding its customer base or increasing sales. You can also compare the company's revenue growth rate to that of its competitors to see how it stacks up.\n",
            "3. Examine profit trends: Profit is the amount of money a company earns after subtracting its expenses. Look at the company's profit growth rate over the past few years. A consistently growing profit trend is a good sign, as it indicates that the company is able to increase its earnings despite rising costs. You can also compare the company's profit growth rate to that of its competitors to see how it compares.\n",
            "4. Assess profit margin trends: Profit margin is the percentage of revenue that a company keeps as profit. Look at the company's profit margin trend over the past few years. A consistently increasing profit margin is a positive sign, as it indicates that the company is able to increase its profitability even as revenue grows. A declining profit margin, on the other hand, could be a red flag, as it may indicate that the company is facing increasing costs or decreasing pricing power.\n",
            "5. Consider other factors: While revenue, profit, and profit margin trends are important indicators of a company's financial health, they are not the only factors to consider. Look at other financial metrics, such as debt levels, cash flow, and earnings per share, to get a more complete picture of the company's financial situation.\n",
            "6. Evaluate the industry: Consider the overall industry trends and how they may impact the company's financial performance. For example, a company in a growing industry may have more growth potential than a company in a mature industry.\n",
            "7. Assess the competition: Look at how the company compares to its competitors in terms of financial performance, market share, and competitive advantages.\n",
            "8. Consider valuation: Finally, consider the company's valuation relative to its earnings and growth potential. A stock that looks cheap based on its valuation metrics may be a good buy, while an overvalued stock may not be worth the investment.\n",
            "\n",
            "By following these steps, you can gain a better understanding of a company's financial health and potential for growth based on its revenue, profit, and profit margin trends. However, keep in mind that investing in stocks always carries risk, and it's important to do your own research and consult with a financial advisor before making any investment decisions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama from Hugging Facehub\n",
        "https://huggingface.co/meta-llama\n",
        "\n",
        "* You need to fill the contact info and wait for the approval.\n",
        "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B"
      ],
      "metadata": {
        "id": "3IUaxgdbnOhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "#Throws an error\n",
        "#The model meta-llama/Meta-Llama-3.1-8B is too large to be loaded automatically (16GB > 10GB).\n",
        "#Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id,\n",
        "    model_kwargs={\"temperature\": 0.9},\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"What are some ways to boost creativity?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "2bJ8n1APujqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473e09e9-12a2-4116-8e4d-896739865620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are some ways to boost creativity? Many people believe that there is a limit to one’s creativity and that it is either something that you’re born with or something that you can’t do much to change. However, research has shown that there are several ways to boost creativity and improve your ability to generate new and innovative ideas. Here are some tips:\n",
            "\n",
            "1. Diversify your experiences: Engage in new activities, travel to new places, and meet new people. New experiences can broaden your perspective and give you new ideas and inspiration.\n",
            "2. Practice creative thinking: Make a habit of thinking creatively in your daily life. Look for new ways to solve problems, approach tasks from different angles, and challenge assumptions.\n",
            "3. Take breaks: Give yourself time to relax and let your mind wander. Taking breaks can help you recharge and come up with new ideas.\n",
            "4. Get enough sleep: Lack of sleep can negatively impact your ability to think creatively. Make sure you’re getting enough rest each night.\n",
            "5. Exercise regularly: Physical activity can help improve brain function and boost creativity.\n",
            "6. Eat a healthy diet: Eating a nutritious diet can help improve brain function and boost energy levels, which can help you be more creative.\n",
            "7. Manage stress: High levels of stress can negatively impact your ability to think creatively. Find ways to manage stress, such as practicing relaxation techniques or engaging in activities that you enjoy.\n",
            "8. Surround yourself with creative people: Being around other creative people can be inspiring and help you feel more motivated to be creative yourself.\n",
            "9. Keep a journal: Keeping a journal can help you capture new ideas as they come to you. Write down your thoughts, ideas, and inspirations as they arise.\n",
            "10. Take risks: Being open to new experiences and taking risks can help you discover new things and come up with creative solutions to problems.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Replicate"
      ],
      "metadata": {
        "id": "A6lV6zlWWVsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Run and fine-tune open-source models with Replicate's API.https://replicate.com/home\n",
        "- Deploy custom models at scale using one line of code.\n",
        "- Avoid managing infrastructure or learning machine learning details.\n",
        "- Use open-source models or package your own.\n",
        "- Choose to make models public or keep them private.\n",
        "- Start with any open-source model with just one line of code.\n"
      ],
      "metadata": {
        "id": "qPL03Yfnr7Vx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replciate API Token\n",
        "\n",
        "On top Left >>> Home>>Click on your id>> API Tokens\n",
        "https://replicate.com/account/api-tokens"
      ],
      "metadata": {
        "id": "uZMqrpD6sKWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install replicate"
      ],
      "metadata": {
        "id": "UhKCAyesWZlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"REPLICATE_API_TOKEN\"] = userdata.get(\"REPLICATE_API_TOKEN\")"
      ],
      "metadata": {
        "id": "EyQu5O3VsOd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import Replicate\n",
        "\n",
        "replicate_llm = Replicate(\n",
        "    model=\"meta/meta-llama-3.1-405b-instruct\",\n",
        "    model_kwargs={\"temperature\": 0.6},\n",
        ")\n",
        "\n",
        "response = replicate_llm.invoke(\"What are some good strategies for studying?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "g8cuUoGAs9di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Groq"
      ],
      "metadata": {
        "id": "nZ9G5F5DWTqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Developed the LPU(Language Processing Unit) chip to run LLMs faster and cheaper.\n",
        "* Offers Groq Cloud to try open-source LLMs like Llama3 or Mixtral.\n",
        "* Allows free use of Llama3 or Mixtral in apps via Groq API Key with rate limits.\n",
        "* Models on Groq https://console.groq.com/docs/models\n",
        "* Get your Groq API key https://console.groq.com/keys\n"
      ],
      "metadata": {
        "id": "qq-Y3ae0oX7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq"
      ],
      "metadata": {
        "id": "ilkVBfuso1jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")"
      ],
      "metadata": {
        "id": "8mohUh45uJrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mLICC6MpWVLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm=ChatGroq(\n",
        "    model=\"llama3-70b-8192\"\n",
        ")\n",
        "result=llm.invoke(\"what are the top 10 quotes about ignorance?\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "3JCgb4ubpAy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Many more ways"
      ],
      "metadata": {
        "id": "KDcBv9MtlU5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://python.langchain.com/v0.1/docs/integrations/llms/"
      ],
      "metadata": {
        "id": "Mr1NfzpUlXY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#invoke models using inference end points"
      ],
      "metadata": {
        "id": "7c4JPXjq1eBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#Better way\n",
        "from google.colab import userdata\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")"
      ],
      "metadata": {
        "id": "nJUuOYO-uqtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade huggingface_hub -q"
      ],
      "metadata": {
        "id": "-PdLL4sgt59s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "client = InferenceClient(\n",
        "\tprovider=\"hf-inference\",\n",
        "\tapi_key=os.environ['HUGGINGFACEHUB_API_TOKEN']\n",
        ")\n",
        "\n",
        "messages = [\n",
        "\t{\n",
        "\t\t\"role\": \"user\",\n",
        "\t\t\"content\": \"What is the capital of France?\"\n",
        "\t}\n",
        "]\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
        "\tmessages=messages,\n",
        "\tmax_tokens=500\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDzmyiOTht3R",
        "outputId": "ad0c5384-61e8-483e-9f88-5e6a720d9b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionOutputMessage(role='assistant', content='<think>\\n\\n</think>\\n\\nThe capital of France is Paris.', tool_calls=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "vOOZ01smiR1e",
        "outputId": "09d4795d-8c25-4dd0-a1b2-1d1f315c136e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.27.1)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2024.12.14)\n",
            "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.27.1\n",
            "    Uninstalling huggingface-hub-0.27.1:\n",
            "      Successfully uninstalled huggingface-hub-0.27.1\n",
            "Successfully installed huggingface_hub-0.28.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub"
                ]
              },
              "id": "abd1455c20eb43e4ad150d8653f74e00"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}